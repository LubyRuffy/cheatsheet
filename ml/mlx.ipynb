{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# mlx\n",
    "mlx是苹果官方出的机器学习框架，能够充分利用apple silicon芯片的性能，比pytorch+mps的模式要快一点。\n",
    "\n",
    "## 下载安装\n",
    "### 安装mlx-lm\n",
    "```shell\n",
    "pip install mlx-lm\n",
    "```\n",
    "\n",
    "## 开箱即用的测试\n",
    "因为默认就可以用gguf，所以可以直接预测：\n",
    "```shell\n",
    "python -m mlx_lm.generate --model mlx-community/phi-2-hf-4bit-mlx --prompt \"hello\"\n",
    "```\n",
    "可以通过一些参数（seed和temp\n",
    "```shell\n",
    "python -m mlx_lm.generate --model mlx-community/quantized-gemma-2b-it --prompt \"你是谁？\" --temp 0.95 --seed `date +%s`\n",
    "```\n",
    "因为phi-2是一个基础模型，而不是chat模型，所以默认给出的是拼凑hello world的续写，想要给出对话响应，可以用类似这样的prompt：\n",
    "```shell\n",
    "python -m mlx_lm.generate --model mlx-community/phi-2-hf-4bit-mlx --prompt \"User: hello\n",
    "AI:\" --colorize --ignore-chat-template\n",
    "```\n",
    "\n",
    "## 微调\n",
    "### 准备好数据\n",
    "拿`LLaMA-Factory/blob/main/data/identity.json`的标识来进行测试，以前叫self_cognition自我认知。\n",
    "数据格式类似于：\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"instruction\": \"hi\",\n",
    "    \"input\": \"\",\n",
    "    \"output\": \"Hello! I am NAME, an AI assistant developed by AUTHOR. How can I assist you today?\"\n",
    "  },\n",
    "  {\n",
    "    \"instruction\": \"hello\",\n",
    "    \"input\": \"\",\n",
    "    \"output\": \"Hello! I am NAME, an AI assistant developed by AUTHOR. How can I assist you today?\"\n",
    "  },\n",
    "]\n",
    "```\n",
    "把NAME和AUTHOR进行批量替换：\n",
    "```shell\n",
    "wget https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/data/identity.json\n",
    "sed -i '' 's/NAME/FOFABot/g' identity.json\n",
    "sed -i '' 's/AUTHOR/华顺信安/g' identity.json\n",
    "```\n",
    "这时候看到数据如下：\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"instruction\": \"hi\",\n",
    "    \"input\": \"\",\n",
    "    \"output\": \"Hello! I am FOFABot, an AI assistant developed by 华顺信安. How can I assist you today?\"\n",
    "  },\n",
    "  {\n",
    "    \"instruction\": \"hello\",\n",
    "    \"input\": \"\",\n",
    "    \"output\": \"Hello! I am FOFABot, an AI assistant developed by 华顺信安. How can I assist you today?\"\n",
    "  },\n",
    "]\n",
    "```\n",
    "\n",
    "生成mlx需要的格式，要结合待微调的模型来进行定制，比如yi和deepseek就不一样，base模型和chat模型也不一样，生成的text字段要根据模型来进行生成。关于datasets的处理[参考](https://huggingface.co/docs/datasets/en/process)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1b05972a5aa28b5"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['output', 'input', 'instruction'],\n",
      "        num_rows: 91\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['output', 'input', 'instruction', 'text'],\n",
      "        num_rows: 91\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 91\n",
      "    })\n",
      "})\n",
      "{'text': 'Instruction: hi\\nOutput: Hello! I am FOFABot, an AI assistant developed by 华顺信安. How can I assist you today?'}\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 81\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 385.97ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 800.59ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "1958"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install datasets\n",
    "import datasets\n",
    "\n",
    "# 加载 JSONL 文件\n",
    "dataset = datasets.load_dataset(\"json\", data_files=\"identity.json\")\n",
    "print(dataset)\n",
    "\n",
    "# Create the text field using map function\n",
    "dataset = dataset.map(\n",
    "    lambda x: {\"text\": f\"Instruction: {x['instruction']}\\nOutput: {x['output']}\"}\n",
    ")\n",
    "print(dataset)\n",
    "\n",
    "dataset = dataset.remove_columns([\"instruction\", \"input\", \"output\"])\n",
    "print(dataset)\n",
    "\n",
    "print(dataset['train'][0])\n",
    "\n",
    "dataset = dataset['train'].train_test_split(test_size=0.1)\n",
    "print(dataset)\n",
    "\n",
    "# Save the converted \"train\" split as a new JSONL file\n",
    "dataset['train'].to_json('data/train.jsonl', orient='records', lines=True, force_ascii=False)\n",
    "dataset['test'].to_json('data/valid.jsonl', orient='records', lines=True, force_ascii=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T10:43:45.981027Z",
     "start_time": "2024-03-03T10:43:45.168092Z"
    }
   },
   "id": "3cce764674ff837a",
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": [
    "如下代码可以查看需要生成的训练数据格式：\n",
    "```shell\n",
    "MODEL=deepseek-ai/deepseek-coder-7b-instruct-v1.5 python -c 'import os; from transformers import AutoTokenizer; tokenizer = AutoTokenizer.from_pretrained(os.environ[\"MODEL\"]); print(\"=====>>>>\\n\" + tokenizer.apply_chat_template([{\"role\":\"user\",\"content\":\"hi\"}], tokenize=False, add_generation_prompt=True))'\n",
    "```\n",
    "通过MODEL来修改，比如还可以是`Qwen/Qwen1.5-1.8B-Chat`， 或者mlx官方的`mlx-community/phi-2-hf-4bit-mlx`。如何没有配置，会提示`No chat template is defined for this tokenizer - using a default chat template that implements the ChatML format (without BOS/EOS tokens!). If the default is not appropriate for your model, please set tokenizer.chat_template to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.`\n",
    "\n",
    "### 开始微调\n",
    "```\n",
    "python -m mlx_lm.lora --model mlx-community/phi-2-hf-4bit-mlx --train --data ./data --adapter-file fofabot.npz --learning-rate 1e-4\n",
    "```\n",
    "参数中，由于数据量比较小，把learning-rate可以适当调大一点没有问题。\n",
    "\n",
    "### 预测\n",
    "```\n",
    "python -m mlx_lm.lora --model mlx-community/phi-2-hf-4bit-mlx --adapter-file fofabot.npz --prompt \"Instruction: hi\n",
    "Output: \"\n",
    "\n",
    "# 也可以在微调过程中进行验证，比如100步之后可以使用：checkpoints/100_fofabot.npz \n",
    "python -m mlx_lm.lora --model mlx-community/phi-2-hf-4bit-mlx --adapter-file checkpoints/100_fofabot.npz --prompt \"Instruction: hi\n",
    "Output: \"\n",
    "```\n",
    "训练1000次后的回答是：\n",
    "```\n",
    "作为 FOFABot，我的核心价值是致力、问题检索和给信息提供。\n",
    "```\n",
    "\n",
    "用非量化的版本`microsoft/phi-2`训练100次后就马上看到了效果,不过还有乱码:\n",
    "```\n",
    "你好，我是 FOFABot，很高兴认识你。\n",
    "!@#$%^&*()_+;<=>?@rTTYUIOP{}|ASDFGHJKL:\"'''\n",
    "```\n",
    "\n",
    "训练1000次后,还有尾巴:\n",
    "```\n",
    "你好，我是 FOFABot，很高兴为您服务。有什么我可以帮您解决的问题或者需要我提供的帮助吗？! # FOFABot，很高\n",
    "```\n",
    "用llama-factory训练后是好的。\n",
    "\n",
    "### qwen微调\n",
    "上面看起来怪怪的，不知道是不是因为base模型的关系，用非量化的版本再测试一下。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d62fee4a0dd8901"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'output', 'instruction'],\n",
      "        num_rows: 91\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/91 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "95387780fd2c47f8aff38607edb8c080"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'output', 'instruction', 'text'],\n",
      "        num_rows: 91\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 91\n",
      "    })\n",
      "})\n",
      "{'text': '<|im_start|>user\\nhi<|im_end|>\\n<|im_start|>assistant\\nHello! I am FOFABot, an AI assistant developed by 华顺信安. How can I assist you today?<|im_end|>\\n'}\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 81\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9a7ebfee1ebc4b538dbc95e34e93518b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8cb93cf01fa541f889af3e117a461841"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "2194"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "# 加载 JSONL 文件\n",
    "dataset = datasets.load_dataset(\"json\", data_files=\"identity.json\")\n",
    "print(dataset)\n",
    "\n",
    "# Create the text field using map function\n",
    "dataset = dataset.map(\n",
    "    lambda x: {\"text\": f\"<|im_start|>user\\n{x['instruction']}<|im_end|>\\n<|im_start|>assistant\\n{x['output']}<|im_end|>\\n\"}\n",
    ")\n",
    "print(dataset)\n",
    "\n",
    "dataset = dataset.remove_columns([\"instruction\", \"input\", \"output\"])\n",
    "print(dataset)\n",
    "\n",
    "print(dataset['train'][0])\n",
    "\n",
    "dataset = dataset['train'].train_test_split(test_size=0.1)\n",
    "print(dataset)\n",
    "\n",
    "# Save the converted \"train\" split as a new JSONL file\n",
    "dataset['train'].to_json('qwendata/train.jsonl', orient='records', lines=True, force_ascii=False)\n",
    "dataset['test'].to_json('qwendata/valid.jsonl', orient='records', lines=True, force_ascii=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T13:42:08.139979Z",
     "start_time": "2024-03-03T13:42:07.070276Z"
    }
   },
   "id": "2b023509ff4815c7",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "```shell\n",
    "python -m mlx_lm.lora --model Qwen/Qwen1.5-1.8B-Chat --train --data ./qwendata --adapter-file qwen_fofabot.npz --learning-rate 1e-4\n",
    "\n",
    "python -m mlx_lm.lora --model Qwen/Qwen1.5-1.8B-Chat --adapter-file checkpoints/100_qwen_fofabot.npz --prompt \"<|im_start|>user\n",
    "hi<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\n",
    "```\n",
    "100次效果就非常好了:\n",
    "```\n",
    "Hello! I am FOFABot, an AI assistant developed by 华顺信安. How can I assist you today?\n",
    "```\n",
    "\n",
    "训练速度非常快,训练完成的效果:\n",
    "```\n",
    "python -m mlx_lm.lora --model Qwen/Qwen1.5-1.8B-Chat --adapter-file qwen_fofabot.npz --prompt \"<|im_start|>user\n",
    "你是谁?<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\n",
    "\n",
    "您好，我是 FOFABot，由 华顺信安 开发，旨在为用户提供智能化的回答和帮助。\n",
    "```\n",
    "\n",
    "## 数据格式\n",
    "根据不同的模型来生成文件，mlx的约定为：\n",
    "- 指定目录，目录下有不同的jsonl文件\n",
    "- 必须为jsonl的格式，每行一个json，只处理text字段\n",
    "- 需要至少有train和valid两个文件，一个用于训练一个用于验证\n",
    "\n",
    "## 更多参考：\n",
    "- [Fine-tuning the latest Google Gemma model locally using MLX](https://gist.github.com/alexweberk/635431b5c5773efd6d1755801020429f)\n",
    "    - https://github.com/alexweberk/playing-with-llms/blob/main/notebooks/mlx_gemma/mlx_finetuning_gemma.ipynb\n",
    "- [MLX LLM Finetuning](https://github.com/AaronWard/generative-ai-workbook/blob/main/personal_projects/23.mlx-finetuning/1.fine-tuning.ipynb)\n",
    "- [sql-create-context-mlx-lora](https://github.com/alwint3r/sql-create-context-mlx-lora)\n",
    "- [chat-with-mlx](https://github.com/qnguyen3/chat-with-mlx)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2036c4df51d83619"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f7246eab279b50d3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
