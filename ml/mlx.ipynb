{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# mlx\n",
    "\n",
    "## 基础知识\n",
    "mlx是苹果官方出的机器学习框架，能够充分利用apple silicon芯片的性能，比pytorch+mps的模式要快一点。\n",
    "\n",
    "### 安装mlx-lm\n",
    "```shell\n",
    "pip install mlx-lm\n",
    "```\n",
    "\n",
    "### 开箱即用的测试\n",
    "因为默认就可以用gguf，所以可以直接预测：\n",
    "```shell\n",
    "mlx_lm.generate --model mlx-community/phi-2-hf-4bit-mlx --prompt \"hello\"\n",
    "```\n",
    "可以通过一些参数（seed和temp\n",
    "```shell\n",
    "mlx_lm.generate --model mlx-community/quantized-gemma-2b-it --prompt \"你是谁？\" --temp 0.95 --seed `date +%s`\n",
    "```\n",
    "因为phi-2是一个基础模型，而不是chat模型，所以默认给出的是拼凑hello world的续写，想要给出对话响应，可以用类似这样的prompt：\n",
    "```shell\n",
    "mlx_lm.generate --model mlx-community/phi-2-hf-4bit-mlx --prompt \"User: hello\n",
    "AI:\" --colorize --ignore-chat-template\n",
    "```\n",
    "\n",
    "### mlx_lm模块内嵌的几个命令\n",
    "\n",
    "#### convert\n",
    "转换safetensors到mlx格式\n",
    "```shell\n",
    "mlx_lm.convert --hf-path mistralai/Mistral-7B-v0.1 -q \n",
    "```\n",
    "如果只有pytorch格式的文件，会报错：`FileNotFoundError: No safetensors found`\n",
    "\n",
    "#### generate\n",
    "生成\n",
    "\n",
    "#### lora\n",
    "微调\n",
    "\n",
    "### 微调\n",
    "#### 准备好数据\n",
    "拿`LLaMA-Factory/blob/main/data/identity.json`的标识来进行测试，以前叫self_cognition自我认知。\n",
    "数据格式类似于：\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"instruction\": \"hi\",\n",
    "    \"input\": \"\",\n",
    "    \"output\": \"Hello! I am NAME, an AI assistant developed by AUTHOR. How can I assist you today?\"\n",
    "  },\n",
    "  {\n",
    "    \"instruction\": \"hello\",\n",
    "    \"input\": \"\",\n",
    "    \"output\": \"Hello! I am NAME, an AI assistant developed by AUTHOR. How can I assist you today?\"\n",
    "  },\n",
    "]\n",
    "```\n",
    "把NAME和AUTHOR进行批量替换：\n",
    "```shell\n",
    "wget https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/data/identity.json\n",
    "sed -i '' 's/NAME/FOFABot/g' identity.json\n",
    "sed -i '' 's/AUTHOR/华顺信安/g' identity.json\n",
    "```\n",
    "这时候看到数据如下：\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"instruction\": \"hi\",\n",
    "    \"input\": \"\",\n",
    "    \"output\": \"Hello! I am FOFABot, an AI assistant developed by 华顺信安. How can I assist you today?\"\n",
    "  },\n",
    "  {\n",
    "    \"instruction\": \"hello\",\n",
    "    \"input\": \"\",\n",
    "    \"output\": \"Hello! I am FOFABot, an AI assistant developed by 华顺信安. How can I assist you today?\"\n",
    "  },\n",
    "]\n",
    "```\n",
    "\n",
    "备注：在mlx_lm 0.5.0 版本开始，已经支持chat对话格式了，不用手动配置。参考[mlx_lm的LORA说明](https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/LORA.md)  \n",
    "\n",
    "生成mlx需要的格式，要结合待微调的模型来进行定制，比如yi和deepseek就不一样，base模型和chat模型也不一样，生成的text字段要根据模型来进行生成。关于datasets的处理[参考](https://huggingface.co/docs/datasets/en/process)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1b05972a5aa28b5"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['output', 'input', 'instruction'],\n",
      "        num_rows: 91\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['output', 'input', 'instruction', 'text'],\n",
      "        num_rows: 91\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 91\n",
      "    })\n",
      "})\n",
      "{'text': 'Instruction: hi\\nOutput: Hello! I am FOFABot, an AI assistant developed by 华顺信安. How can I assist you today?'}\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 81\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 385.97ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 800.59ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "1958"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install datasets\n",
    "import datasets\n",
    "\n",
    "# 加载 JSONL 文件\n",
    "dataset = datasets.load_dataset(\"json\", data_files=\"identity.json\")\n",
    "print(dataset)\n",
    "\n",
    "# Create the text field using map function\n",
    "dataset = dataset.map(\n",
    "    lambda x: {\"text\": f\"Instruction: {x['instruction']}\\nOutput: {x['output']}\"}\n",
    ")\n",
    "print(dataset)\n",
    "\n",
    "dataset = dataset.remove_columns([\"instruction\", \"input\", \"output\"])\n",
    "print(dataset)\n",
    "\n",
    "print(dataset['train'][0])\n",
    "\n",
    "dataset = dataset['train'].train_test_split(test_size=0.1)\n",
    "print(dataset)\n",
    "\n",
    "# Save the converted \"train\" split as a new JSONL file\n",
    "dataset['train'].to_json('data/train.jsonl', orient='records', lines=True, force_ascii=False)\n",
    "dataset['test'].to_json('data/valid.jsonl', orient='records', lines=True, force_ascii=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T10:43:45.981027Z",
     "start_time": "2024-03-03T10:43:45.168092Z"
    }
   },
   "id": "3cce764674ff837a",
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": [
    "如下代码可以查看需要生成的训练数据格式：\n",
    "```shell\n",
    "MODEL=deepseek-ai/deepseek-coder-7b-instruct-v1.5 python -c 'import os; from transformers import AutoTokenizer; tokenizer = AutoTokenizer.from_pretrained(os.environ[\"MODEL\"]); print(\"=====>>>>\\n\" + tokenizer.apply_chat_template([{\"role\":\"user\",\"content\":\"hi\"}], tokenize=False, add_generation_prompt=True))'\n",
    "```\n",
    "通过MODEL来修改，比如还可以是`Qwen/Qwen1.5-1.8B-Chat`， 或者mlx官方的`mlx-community/phi-2-hf-4bit-mlx`。如何没有配置，会提示`No chat template is defined for this tokenizer - using a default chat template that implements the ChatML format (without BOS/EOS tokens!). If the default is not appropriate for your model, please set tokenizer.chat_template to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.`\n",
    "\n",
    "### 开始微调\n",
    "```\n",
    "mlx_lm.lora --model mlx-community/phi-2-hf-4bit-mlx --train --data ./data --adapter-file fofabot.npz --learning-rate 1e-4\n",
    "```\n",
    "参数中，由于数据量比较小，把learning-rate可以适当调大一点没有问题。\n",
    "\n",
    "### 预测\n",
    "```\n",
    "mlx_lm.lora --model mlx-community/phi-2-hf-4bit-mlx --adapter-file fofabot.npz --prompt \"Instruction: hi\n",
    "Output: \"\n",
    "\n",
    "# 也可以在微调过程中进行验证，比如100步之后可以使用：checkpoints/100_fofabot.npz \n",
    "mlx_lm.lora --model mlx-community/phi-2-hf-4bit-mlx --adapter-file checkpoints/100_fofabot.npz --prompt \"Instruction: hi\n",
    "Output: \"\n",
    "```\n",
    "训练1000次后的回答是：\n",
    "```\n",
    "作为 FOFABot，我的核心价值是致力、问题检索和给信息提供。\n",
    "```\n",
    "\n",
    "用非量化的版本`microsoft/phi-2`训练100次后就马上看到了效果,不过还有乱码:\n",
    "```\n",
    "你好，我是 FOFABot，很高兴认识你。\n",
    "!@#$%^&*()_+;<=>?@rTTYUIOP{}|ASDFGHJKL:\"'''\n",
    "```\n",
    "\n",
    "训练1000次后,还有尾巴:\n",
    "```\n",
    "你好，我是 FOFABot，很高兴为您服务。有什么我可以帮您解决的问题或者需要我提供的帮助吗？! # FOFABot，很高\n",
    "```\n",
    "用llama-factory训练后是好的。\n",
    "\n",
    "### qwen微调\n",
    "上面看起来怪怪的，不知道是不是因为base模型的关系，用非量化的版本再测试一下。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d62fee4a0dd8901"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'output', 'instruction'],\n",
      "        num_rows: 91\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/91 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "95387780fd2c47f8aff38607edb8c080"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'output', 'instruction', 'text'],\n",
      "        num_rows: 91\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 91\n",
      "    })\n",
      "})\n",
      "{'text': '<|im_start|>user\\nhi<|im_end|>\\n<|im_start|>assistant\\nHello! I am FOFABot, an AI assistant developed by 华顺信安. How can I assist you today?<|im_end|>\\n'}\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 81\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9a7ebfee1ebc4b538dbc95e34e93518b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8cb93cf01fa541f889af3e117a461841"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "2194"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "# 加载 JSONL 文件\n",
    "dataset = datasets.load_dataset(\"json\", data_files=\"identity.json\")\n",
    "print(dataset)\n",
    "\n",
    "# Create the text field using map function\n",
    "dataset = dataset.map(\n",
    "    lambda x: {\"text\": f\"<|im_start|>user\\n{x['instruction']}<|im_end|>\\n<|im_start|>assistant\\n{x['output']}<|im_end|>\\n\"}\n",
    ")\n",
    "print(dataset)\n",
    "\n",
    "dataset = dataset.remove_columns([\"instruction\", \"input\", \"output\"])\n",
    "print(dataset)\n",
    "\n",
    "print(dataset['train'][0])\n",
    "\n",
    "dataset = dataset['train'].train_test_split(test_size=0.1)\n",
    "print(dataset)\n",
    "\n",
    "# Save the converted \"train\" split as a new JSONL file\n",
    "dataset['train'].to_json('qwendata/train.jsonl', orient='records', lines=True, force_ascii=False)\n",
    "dataset['test'].to_json('qwendata/valid.jsonl', orient='records', lines=True, force_ascii=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T13:42:08.139979Z",
     "start_time": "2024-03-03T13:42:07.070276Z"
    }
   },
   "id": "2b023509ff4815c7",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "```shell\n",
    "mlx_lm.lora --model Qwen/Qwen1.5-1.8B-Chat --train --data ./qwendata --adapter-file qwen_fofabot.npz --learning-rate 1e-4\n",
    "\n",
    "mlx_lm.lora --model Qwen/Qwen1.5-1.8B-Chat --adapter-file checkpoints/100_qwen_fofabot.npz --prompt \"<|im_start|>user\n",
    "hi<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\n",
    "```\n",
    "100次效果就非常好了:\n",
    "```\n",
    "Hello! I am FOFABot, an AI assistant developed by 华顺信安. How can I assist you today?\n",
    "```\n",
    "\n",
    "训练速度非常快,训练完成的效果:\n",
    "```\n",
    "mlx_lm.lora --model Qwen/Qwen1.5-1.8B-Chat --adapter-file qwen_fofabot.npz --prompt \"<|im_start|>user\n",
    "你是谁?<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\n",
    "\n",
    "您好，我是 FOFABot，由 华顺信安 开发，旨在为用户提供智能化的回答和帮助。\n",
    "```\n",
    "\n",
    "### 数据格式\n",
    "根据不同的模型来生成文件，mlx的约定为：\n",
    "- 指定目录，目录下有不同的jsonl文件\n",
    "- 必须为jsonl的格式，每行一个json，只处理text字段\n",
    "- 需要至少有train和valid两个文件，一个用于训练一个用于验证\n",
    "\n",
    "### 更多参考：\n",
    "- [Fine-tuning the latest Google Gemma model locally using MLX](https://gist.github.com/alexweberk/635431b5c5773efd6d1755801020429f)\n",
    "    - https://github.com/alexweberk/playing-with-llms/blob/main/notebooks/mlx_gemma/mlx_finetuning_gemma.ipynb\n",
    "- [MLX LLM Finetuning](https://github.com/AaronWard/generative-ai-workbook/blob/main/personal_projects/23.mlx-finetuning/1.fine-tuning.ipynb)\n",
    "- [sql-create-context-mlx-lora](https://github.com/alwint3r/sql-create-context-mlx-lora)\n",
    "- [chat-with-mlx](https://github.com/qnguyen3/chat-with-mlx)\n",
    "\n",
    "## 场景任务\n",
    "### 从huggingface的pytorch模型转换为支持mlx训练的模型\n",
    "mlx可以调用的格式默认为safetensors的模型，类似于deepseek-ai/deepseek-coder-33b-instruct这样的没有导致generate失败：\n",
    "```shell\n",
    "mlx_lm.generate --model deepseek-ai/deepseek-coder-7b-instruct-v1.5 --prompt \"用golang写一个针对目标ip的tcp端口扫描器\" --temp 0.95 --seed `date +%s` -m 1024 --trust-remote-code\n",
    "# 这个是可以正常输出的，因为https://huggingface.co/deepseek-ai/deepseek-coder-7b-instruct-v1.5/tree/main下面是.safetensors格式\n",
    "\n",
    "mlx_lm.generate --model deepseek-ai/deepseek-coder-33b-instruct --prompt \"用golang写一个针对目标ip的tcp端口扫描器\" --temp 0.95 --seed `date +%s` -m 1024 --trust-remote-code\n",
    "# FileNotFoundError: No safetensors found in xxx/huggingface/hub/models--deepseek-ai--deepseek-coder-33b-instruct/snapshots/xxx\n",
    "# deepseek-ai/deepseek-coder-33b-instruct 里面都是pytorch的bin，需要转换\n",
    "```\n",
    "转换未完成。\n",
    "\n",
    "### 根据模型提示词\n",
    "```shell\n",
    "MODEL=mlx-community/quantized-gemma-7b-it python -c 'import os; from transformers import AutoTokenizer; tokenizer = AutoTokenizer.from_pretrained(os.environ[\"MODEL\"]); print(\"=====>>>>\\n\" + tokenizer.apply_chat_template([{\"role\":\"system\",\"content\":\"system_prompt\"},{\"role\":\"user\",\"content\":\"user_prompt\"},{\"role\":\"assistant\",\"content\":\"assistant_response\"}], tokenize=False, add_generation_prompt=False))'\n",
    "```\n",
    "如果不存在system角色，会提示：`jinja2.exceptions.TemplateError: System role not supported`\n",
    "\n",
    "```shell\n",
    "MODEL=mlx-community/Yi-6B-Chat-hf-4bit-mlx python -c 'import os; from transformers import AutoTokenizer; tokenizer = AutoTokenizer.from_pretrained(os.environ[\"MODEL\"]); print(\"=====>>>>\\n\" + tokenizer.apply_chat_template([{\"role\":\"system\",\"content\":\"system_prompt\"},{\"role\":\"user\",\"content\":\"user_prompt\"},{\"role\":\"assistant\",\"content\":\"assistant_response\"}], tokenize=False, add_generation_prompt=False))'\n",
    "```\n",
    "```text\n",
    "<|im_start|>system\n",
    "system_prompt<|im_end|>\n",
    "<|im_start|>user\n",
    "user_prompt<|im_end|>\n",
    "<|im_start|>assistant\n",
    "assistant_response<|im_end|>\n",
    "```\n",
    "\n",
    "## 常见问题\n",
    "\n",
    "### 如何设定系统提示词？\n",
    "mlx没有做文本内容的处理，所以在generate的时候默认会套用模型的chat tempalte，比如：\n",
    "```shell\n",
    "mlx_lm.generate --model deepseek-ai/deepseek-coder-7b-instruct-v1.5 --prompt \"hi\"\n",
    "```\n",
    "会生成这样的Prompt：\n",
    "```\n",
    "Prompt: <｜begin▁of▁sentence｜>You are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer\n",
    "### Instruction:\n",
    "hi\n",
    "### Response:\n",
    "```\n",
    "我们可以通过`--ignore-chat-template`参数来自己设定，比如：\n",
    "```shell\n",
    "mlx_lm.generate --model deepseek-ai/deepseek-coder-7b-instruct-v1.5 --prompt \"Prompt: <｜begin▁of▁sentence｜>你是华顺信安公司开发的助理机器人，名叫FOFABot。你要帮助用户解答关于fofa的问题。\n",
    "### Instruction:\n",
    "hi\n",
    "### Response:\" --ignore-chat-template\n",
    "```\n",
    "这时候prompt就变成了：\n",
    "```\n",
    "Prompt: <｜begin▁of▁sentence｜>你是华顺信安公司开发的助理机器人，名叫FOFABot。你要帮助用户解答所有关于fofa的任何问题。\n",
    "### Instruction:\n",
    "hi\n",
    "### Response:\n",
    "```\n",
    "相当于设置了system prompt。\n",
    "\n",
    "### 如何手动编译mlx_lm模块本地安装\n",
    "由于代码更新太快，比如qwen_moe的代码更新了，但是mlx_lm库还没有更新，导致用不了，所以需要手动编译安装。\n",
    "```shell\n",
    "pip install --user --upgrade build\n",
    "pip install --user --upgrade twine\n",
    "cd mlx-examples/llms # 一定要到这个目录\n",
    "python -m build # 编译后放到了dist目录下\n",
    "pip install --force-reinstall  ./dist/mlx_lm-0.6.0-py3-none-any.whl # 重新安装 --force-reinstall 否则会提示mlx-lm is already installed with the same version as the provided wheel\n",
    "python -m mlx_lm.convert --hf-path Qwen/Qwen1.5-MoE-A2.7B-Chat --mlx-path Qwen1.5-MoE-A2.7B-Chat -q # 这时候就可以了\n",
    "```\n",
    "确保transformers也是最新的，可以源代码部署\n",
    "```shell\n",
    "pip install git+https://github.com/huggingface/transformers\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2036c4df51d83619"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f7246eab279b50d3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
