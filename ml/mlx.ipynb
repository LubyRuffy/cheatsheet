{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# mlx\n",
    "mlx是苹果官方出的机器学习框架，能够充分利用apple silicon芯片的性能，比pytorch+mps的模式要快一点。\n",
    "\n",
    "## 下载安装\n",
    "### 安装mlx-lm\n",
    "```shell\n",
    "pip install mlx-lm\n",
    "```\n",
    "\n",
    "## 开箱即用的测试\n",
    "因为默认就可以用gguf，所以可以直接预测：\n",
    "```shell\n",
    "python -m mlx_lm.generate --model mlx-community/phi-2-hf-4bit-mlx --prompt \"hello\"\n",
    "```\n",
    "因为phi-2是一个基础模型，而不是chat模型，所以默认给出的是拼凑hello world的续写，想要给出对话响应，可以用类似这样的prompt：\n",
    "```shell\n",
    "python -m mlx_lm.generate --model mlx-community/phi-2-hf-4bit-mlx --prompt \"User: hello\n",
    "AI:\" --colorize --ignore-chat-template\n",
    "```\n",
    "\n",
    "## 微调\n",
    "### 准备好数据\n",
    "拿`LLaMA-Factory/blob/main/data/identity.json`的标识来进行测试，以前叫self_cognition自我认知。\n",
    "数据格式类似于：\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"instruction\": \"hi\",\n",
    "    \"input\": \"\",\n",
    "    \"output\": \"Hello! I am NAME, an AI assistant developed by AUTHOR. How can I assist you today?\"\n",
    "  },\n",
    "  {\n",
    "    \"instruction\": \"hello\",\n",
    "    \"input\": \"\",\n",
    "    \"output\": \"Hello! I am NAME, an AI assistant developed by AUTHOR. How can I assist you today?\"\n",
    "  },\n",
    "]\n",
    "```\n",
    "把NAME和AUTHOR进行批量替换：\n",
    "```shell\n",
    "wget https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/data/identity.json\n",
    "sed -i '' 's/NAME/FOFABot/g' identity.json\n",
    "sed -i '' 's/AUTHOR/华顺信安/g' identity.json\n",
    "```\n",
    "这时候看到数据如下：\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"instruction\": \"hi\",\n",
    "    \"input\": \"\",\n",
    "    \"output\": \"Hello! I am FOFABot, an AI assistant developed by 华顺信安. How can I assist you today?\"\n",
    "  },\n",
    "  {\n",
    "    \"instruction\": \"hello\",\n",
    "    \"input\": \"\",\n",
    "    \"output\": \"Hello! I am FOFABot, an AI assistant developed by 华顺信安. How can I assist you today?\"\n",
    "  },\n",
    "]\n",
    "```\n",
    "\n",
    "生成mlx需要的格式，要结合待微调的模型来进行定制，比如yi和deepseek就不一样，base模型和chat模型也不一样，生成的text字段要根据模型来进行生成。关于datasets的处理[参考](https://huggingface.co/docs/datasets/en/process)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1b05972a5aa28b5"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['output', 'input', 'instruction'],\n",
      "        num_rows: 91\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['output', 'input', 'instruction', 'text'],\n",
      "        num_rows: 91\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 91\n",
      "    })\n",
      "})\n",
      "{'text': 'Instruction: hi\\nOutput: Hello! I am FOFABot, an AI assistant developed by 华顺信安. How can I assist you today?'}\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 81\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 385.97ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 800.59ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "1958"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install datasets\n",
    "import datasets\n",
    "\n",
    "# 加载 JSONL 文件\n",
    "dataset = datasets.load_dataset(\"json\", data_files=\"identity.json\")\n",
    "print(dataset)\n",
    "\n",
    "# Create the text field using map function\n",
    "dataset = dataset.map(\n",
    "    lambda x: {\"text\": f\"Instruction: {x['instruction']}\\nOutput: {x['output']}\"}\n",
    ")\n",
    "print(dataset)\n",
    "\n",
    "dataset = dataset.remove_columns([\"instruction\", \"input\", \"output\"])\n",
    "print(dataset)\n",
    "\n",
    "print(dataset['train'][0])\n",
    "\n",
    "dataset = dataset['train'].train_test_split(test_size=0.1)\n",
    "print(dataset)\n",
    "\n",
    "# Save the converted \"train\" split as a new JSONL file\n",
    "dataset['train'].to_json('data/train.jsonl', orient='records', lines=True, force_ascii=False)\n",
    "dataset['test'].to_json('data/valid.jsonl', orient='records', lines=True, force_ascii=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T10:43:45.981027Z",
     "start_time": "2024-03-03T10:43:45.168092Z"
    }
   },
   "id": "3cce764674ff837a",
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "### 开始微调\n",
    "```\n",
    "python -m mlx_lm.lora --model mlx-community/phi-2-hf-4bit-mlx --train --data ./data --adapter-file fofabot.npz --learning-rate 1e-4\n",
    "```\n",
    "参数中，由于数据量比较小，把learning-rate可以适当调大一点没有问题。\n",
    "\n",
    "### 预测\n",
    "```\n",
    "python -m mlx_lm.lora --model mlx-community/phi-2-hf-4bit-mlx --adapter-file fofabot.npz --prompt \"Instruction: hi\n",
    "Output: \"\n",
    "\n",
    "# 也可以在微调过程中进行验证，比如100步之后可以使用：checkpoints/100_fofabot.npz \n",
    "python -m mlx_lm.lora --model mlx-community/phi-2-hf-4bit-mlx --adapter-file checkpoints/100_fofabot.npz \n",
    "--prompt \"Instruction: hi\n",
    "Output: \"\n",
    "```\n",
    "训练1000次后的回答是：\n",
    "```\n",
    "作为 FOFABot，我的核心价值是致力、问题检索和给信息提供。\n",
    "```\n",
    "看起来怪怪的，不知道是不是因为量化模型的关系，用非量化的版本再测试一下。\n",
    "\n",
    "## 数据格式\n",
    "根据不同的模型来生成文件，mlx的约定为：\n",
    "- 指定目录，目录下有不同的jsonl文件\n",
    "- 必须为jsonl的格式，每行一个json，只处理text字段\n",
    "- 需要至少有train和valid两个文件，一个用于训练一个用于验证\n",
    "\n",
    "## 更多参考：\n",
    "- [Fine-tuning the latest Google Gemma model locally using MLX](https://gist.github.com/alexweberk/635431b5c5773efd6d1755801020429f)\n",
    "    - https://github.com/alexweberk/playing-with-llms/blob/main/notebooks/mlx_gemma/mlx_finetuning_gemma.ipynb\n",
    "- [MLX LLM Finetuning](https://github.com/AaronWard/generative-ai-workbook/blob/main/personal_projects/23.mlx-finetuning/1.fine-tuning.ipynb)\n",
    "- [sql-create-context-mlx-lora](https://github.com/alwint3r/sql-create-context-mlx-lora)\n",
    "- [chat-with-mlx](https://github.com/qnguyen3/chat-with-mlx)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2036c4df51d83619"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
